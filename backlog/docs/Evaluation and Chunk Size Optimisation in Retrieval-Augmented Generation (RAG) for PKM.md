# Evaluation and Chunk Size Optimisation in Retrieval-Augmented Generation (RAG) for PKM

## Introduction

Personal Knowledge Management (PKM) systems increasingly use Retrieval-Augmented Generation (RAG) to help users by fetching relevant information (e.g. cached web pages or fragments) and feeding it into a language model for context-aware responses. Evaluating the effectiveness of such a system requires assessing both the retrieval component (are we fetching the right passages?) and the generation component (is the LLM using that information to produce high-quality, accurate assistance?). Another key factor is document chunking – how we split web pages into retrievable pieces. Chunk size and strategy impact retrieval accuracy and how efficiently the LLM can consume context without confusion or truncation. This report presents model-agnostic best practices (2023–2025) for evaluating RAG systems and optimising chunk size, with an emphasis on dynamic, noisy web content in a real-world PKM setting. We cover quantitative metrics (precision, recall, MRR, nDCG, etc.), qualitative/user-centric evaluation (usefulness, trust, cognitive load), and practical tips for tuning chunking to balance retrieval recall and LLM context effectiveness.

## Evaluating Retrieval Effectiveness in RAG

Retrieval quality can be measured with classic Information Retrieval metrics, adapted to the passage-level needs of RAG:

• **Precision@K**: The fraction of the top K retrieved passages that are relevant. High precision@K means fewer irrelevant passages are returned. For example, precision@5 = 0.8 means on average 4 of the top 5 chunks are relevant to the query.

• **Recall@K**: The fraction of all relevant passages that are found in the top K. High recall is crucial in RAG – missing a relevant chunk can mean the LLM lacks key facts. For instance, recall@10 = 0.9 indicates 90% of relevant passages appear within the top 10 results.

• **MRR (Mean Reciprocal Rank)**: The average of the reciprocal of the rank of the first relevant result for each query. This metric emphasizes returning at least one relevant passage very early. An MRR of 1.0 would mean the top result is always relevant, whereas a lower MRR indicates users may have to look further down the list for relevant info.

• **nDCG (Normalized Discounted Cumulative Gain)**: A rank-sensitive metric that accounts for multiple relevant documents and their graded relevance. nDCG@K gives more weight to passages higher in the ranking, penalizing relevant info that appears lower down. In traditional web search, nDCG is useful, but in RAG it is often less critical because an LLM can utilize information as long as it's retrieved somewhere in the context window. RAG systems care more that all relevant tokens are retrieved than the exact order, assuming the context window can hold them.

**Limitations of Traditional Metrics**: Standard IR benchmarks (e.g. MSMARCO or MTEB) typically treat entire documents as the unit of retrieval and evaluate ranking order. This doesn't account for RAG-specific needs like partial document relevance or chunking effects. In practice, relevant information might be spread across multiple small chunks, and LLMs are relatively insensitive to the order of relevant chunks in the prompt as long as they are present somewhere. Thus, pure rank metrics can miss the picture for RAG. For example, retrieving two relevant snippets in positions 1 and 5 is as good as positions 1 and 2 for an LLM, as both end up in the context. What matters is covering the necessary ground with minimal extra baggage.

**Token-Level Evaluation**: Recent work suggests evaluating retrieval at a finer granularity than whole documents. One approach is to measure overlap between retrieved text and the ground truth relevant text at the token level. Smith & Troynikov (2024) propose metrics analogous to precision and recall but defined on tokens. They use an LLM to generate a set of test queries and exact relevant excerpts for each from a corpus, then evaluate how well different chunking strategies retrieve those exact relevant tokens. In their framework, a token-level recall measures what fraction of the relevant tokens are retrieved by the system, and a token-level precision measures what fraction of the retrieved tokens are actually relevant. They also define a token-wise Jaccard Index or Intersection-over-Union (IoU) to summarise the trade-off between missing relevant tokens and fetching extra irrelevant tokens. This fine-grained approach captures the coverage of relevant info and the amount of superfluous text fetched. It reflects the reality of RAG: often a "document" is too coarse – we care whether the specific facts needed are present in the retrieved context, and how much distracting text comes along. For instance, if a query asks "What is the capital of France?", an ideal retrieval returns just "The capital of France is Paris." In token-metrics, that would score high: all relevant tokens ("France", "capital", "Paris") are present with almost no extras. A chunk containing an entire paragraph about France (with unrelated details) might still contain "Paris" (high recall) but also many irrelevant tokens (lower precision), which can distract the model or the user.

• **Efficiency vs. Coverage**: The token-level IoU combines precision and recall to reward methods that retrieve relevant information with minimal waste. An IoU of 1.0 means the retrieved text exactly matches the needed info and nothing more (rare in practice), while lower IoU indicates either missing info (lower recall) or lots of extra text (lower precision). By evaluating RAG retrieval on "did we get all the right bits, and how much else did we get?", we can directly compare chunking methods and retrieval setups in terms of useful information per token. In one study, different chunking strategies varied by up to 9% in token-recall, highlighting that how you split documents can significantly affect retrieval success.

**Dynamic and Noisy Content**: In a PKM scenario with cached web pages, data can be messy, unstructured, or filled with boilerplate (navigation menus, ads, etc.). Evaluation should account for this. For example, Smith & Troynikov include "messy text sources" in their evaluation corpora to reflect the noise of web-scraped content. A high retrieval recall on a clean dataset may drop when faced with pages full of irrelevant text, so it's wise to test on a variety of content types. In such cases, token-level precision will penalize methods that frequently pull in off-topic boilerplate. It's important to ensure your evaluation queries cover scenarios where relevant info is hidden among noise – a good retriever (and chunking strategy) should still surface the signal.

**Human Relevance Judgments**: Besides automated metrics, qualitative evaluation of retrieval can be done via user studies or expert annotation. For example, have users rate whether the passages shown alongside their notes were useful or on-topic. If the PKM system suggests a webpage fragment while you write, does the user feel it's relevant and helpful? Such judgments can capture the usefulness of retrieved context in situ, which raw precision/recall might not fully reflect (sometimes a technically relevant snippet might still not be useful if it's too detailed or poorly presented). User-centric metrics might include: percentage of times the user clicked a suggested page (as a proxy for perceived relevance), or how often they copy information from the retrieved snippet into their notes. One could also measure time saved – e.g. do users write their notes faster or with less effort when the system provides retrieval assistance? Lower cognitive load might manifest as less time spent searching manually. These evaluations require human participation, but they ensure the retrieval component is tuned not just to abstract IR metrics but to actual user benefit.

## Evaluating LLM Generation Quality with Retrieved Context

Once relevant passages are retrieved, the generation phase must weave them into a useful, correct output. We need to evaluate the quality of the LLM's responses in context: are answers correct and grounded in the retrieved information? Are they easy for the user to consume?

**Groundedness and Faithfulness**: A key concern in RAG is hallucination – the model might fabricate information not supported by the retrieved documents. Evaluation here focuses on faithfulness: does the model's output stay true to the provided context? Several methods have emerged:

• **RAGAS (Retrieval Augmented Generation Assessment)**: A framework for reference-free evaluation of RAG pipelines. RAGAS introduces a metric often called faithfulness or context adherence. The idea is to have an LLM fact-check its own output against the retrieved context. One implementation is a two-step process: (1) Breakdown the generated answer into discrete statements (the LLM does this without seeing the context, to avoid bias), and (2) Verification where for each statement, another LLM prompt checks whether that statement is supported by the retrieved documents (here the model sees the context but not the original answer, just one statement at a time). Each statement gets a score (e.g. 1 if supported by the context, 0 if contradicted or not found), and these scores average out to an overall faithfulness score. A perfectly grounded answer that only says things found in the retrieval would score 1.0, whereas an answer containing unsupported claims would score lower. This approach doesn't require a human-written reference answer – it uses the retrieved text as the source of truth. It's useful for detecting hallucinations, but it does rely on an LLM to judge itself, which can introduce quirks (e.g. the LLM might be lenient or make errors in evaluation).

• **TruLens Groundedness**: Another automated metric, similar in spirit to RAGAS, provided by the TruLens library. TruLens has an LLM rate each sentence of the answer on how well it's supported by the context, often on a 0–10 scale. It even asks the model to quote supporting text from the context for each answer sentence. The scores are then normalized to [0,1] and aggregated. In essence, this produces a "groundedness" score indicating the degree to which the answer is grounded in retrieved evidence. If the model claims something with no support in the documents, the score for that sentence would be low. These kinds of LLM-as-judge metrics have to be taken with caution (they can fail in subtle ways), but they provide a scalable way to flag likely hallucinations.

• **Chain-of-Thought based Checks (ChainPoll)**: Instead of a single-pass judgment, Galileo's ChainPoll method combines Chain-of-Thought (CoT) reasoning with multiple prompting rounds. The LLM is prompted to explain step-by-step whether an answer is supported by the context, and this is done multiple times (polling) to see if it's consistent. By averaging the outcomes (e.g. 2 "supported" vs 1 "not supported"), it yields a nuanced score (e.g. ~0.67 in that example) and even a textual justification. This method has shown high correlation (~85%) with human evaluations of factuality. The advantage is that it leverages the LLM's reasoning ability to double-check the answer. It's also fast, since multiple judgments can be batched in one API call. Such approaches help pinpoint hallucinations and measure context adherence more robustly than a single yes/no check.

In practice, these automated faithfulness metrics (RAGAS, TruLens, ChainPoll, etc.) allow us to quantitatively track how often the model's output is grounded in retrieved info. For example, you might report that "97% of answers had high groundedness" after a system update, or that one chunking strategy led to fewer hallucinations (because the context was more precise). Many RAG systems also simply provide the sources as part of the answer (like a citation or reference). This not only aids the user but can be checked: e.g. does the answer actually align with the cited source text?

**Answer Correctness and Relevance**: If your RAG system is meant to answer questions or assist writing, you may also evaluate the quality of the output in traditional ways:

• If you have a ground-truth answer or completed note, you can measure overlap with the model's output. For QA tasks, metrics like Exact Match (EM) or F1-score are common. EM is 1 if the model's answer exactly matches the reference answer, F1 measures token-level overlap (useful when answers can have partial correctness). In a recent ChatGPT-based RAG benchmark, models were evaluated on QA datasets using F1 and EM. A high F1 indicates the model not only used the retrieval but actually answered the question correctly and completely. However, in open-ended note-writing support, you often won't have a single "correct" answer. There, qualitative evaluation is needed.

• **Human Evaluation (Usefulness, Coherence)**: Having human reviewers or end-users rate the output is invaluable for aspects like usefulness, clarity, and lack of confusion. For instance, you could ask users: "Did the AI's suggestion help improve your note?", "Did it introduce any incorrect or irrelevant information?", "How much do you trust the AI's content?". These can be done via Likert scale surveys or comparative evaluations (e.g. compare notes written with vs without RAG assistance). Cognitive load can be indirectly assessed by asking users if the AI's references were easy to understand or if they felt overwhelmed by too much information. In research, one might use standard UX questionnaires (like NASA-TLX for cognitive workload) adapted to this scenario. If users report that they had to sift through a lot of unrelated text, that's a sign the retrieval/generation pipeline is not optimally tuned (maybe chunks are too large or not specific enough). On the other hand, if they say the suggestions were spot-on and saved time, that's a success.

• **Trust and Transparency**: Trust is enhanced when the system is transparent about its sources and limitations. An evaluation of trustworthiness might involve checking if the system properly cites sources and if users actually trust those citations. Microsoft notes that many RAG implementations improve user confidence by providing transparent source attribution – i.e. citing the documents that back the answer. When the AI says "According to Source [1]..." and the user can click and verify, it builds trust. Evaluation here could be: do users verify the sources? Do they continue using the suggestions over time (an indication of trust/utility)? A 2025 Microsoft Cloud blog emphasizes that such auditability increases user confidence and aligns with accountability requirements. In high-stakes domains (finance, medical), this is crucial – users and regulators need to see why the AI gave an answer. So one metric of success is simply the presence of citations and perhaps the correctness of those citations (are they actually relevant to the claim made?). There have been cases where generative models made up references; a trustworthy RAG should avoid that.

In summary, evaluate generation on two fronts: (1) objective correctness & faithfulness (via automated metrics or QA-style scoring, plus checking for hallucinations), and (2) user-centric quality (via human judgment on usefulness, clarity, trust). Both quantitative scores and qualitative feedback matter. For example, you might find that a certain chunking strategy yields a higher automated faithfulness score (fewer unsupported statements) and users prefer it because the answers feel more to-the-point – a win-win. On the other hand, an overly narrow retrieval might score well on faithfulness (the model only says things from context) but users might complain the answer is incomplete or not insightful. Balancing these aspects is key: an ideal PKM assistant produces answers that are correct, well-supported, and helpful without overloading or misleading the user.

## Chunk Size Optimisation: Retrieval vs LLM Context Trade-offs

Choosing the right chunk size (and chunking strategy) for your cached web pages is critical to RAG performance. The goal is to chunk documents in a way that maximises relevant content and minimises irrelevant content per chunk. Chunk too large, and each retrieved piece might contain lots of off-topic text (hurting precision and potentially confusing the LLM). Chunk too small, and you might miss context or require many pieces to cover an answer (risking lower recall or fragmenting information).

**Effects of Chunk Size on Retrieval**: Smaller chunks (e.g. a few sentences or a paragraph) tend to yield higher retrieval precision – the embedding or keyword match is more focused, so when you retrieve a chunk, it's more likely to be directly about the query. Harshad S. illustrates that if you ask about "technetium applications", a 25-token chunk might retrieve a sentence precisely mentioning technetium in medical imaging (very on-point), whereas a 1024-token chunk might retrieve a whole page that includes technetium but also lots of extraneous info. However, larger chunks naturally have higher recall potential – they contain more content that could match the query, so it's less likely that a relevant fact is completely missed. In the same example, a larger chunk might include multiple aspects of technetium (applications and challenges in one go) so you don't miss those details. In short:

• Small chunks: finer-grained matching, less noise per chunk → high precision, lower recall per chunk

• Large chunks: more context captured, fewer total chunks needed → high recall, lower precision (each chunk might be only partly relevant)

This is essentially a precision/recall trade-off, which can be quantified by metrics like those discussed. Many practitioners therefore look at a combined metric like F1 or a Context Relevancy score that balances precision and recall. For instance, a moderate chunk size (neither tiny nor huge) might maximise an F1 or relevancy metric by getting most of the query-related info with only minimal extra fluff. In Harshad's experiment, ~200-token chunks struck such a balance for a specific use case.

Notably, sentence-level chunks have emerged as a surprisingly effective default. A recent evaluation by Superlinked found that simply splitting text into sentences outperformed more complex semantic chunking on several QA datasets. The intuition is that a sentence is a natural, self-contained unit of meaning, so embedding whole sentences yields chunks that "make sense" to both retriever and reader. Their tests showed a naive Sentence Splitter (with a certain token limit and overlap) beat a semantic clustering approach that tried to merge sentences based on similarity. They theorised that semantic similarity can be noisy – high embedding similarity might be triggered by common words or topic overlap that doesn't actually mean the content should be in one chunk. In other words, an algorithm grouping text by semantic embeddings might merge bits that aren't truly contextually coherent, whereas splitting by actual sentence boundaries kept things on-topic. This doesn't mean sentence chunks are always best, but it's a useful guideline: ensure chunks align with human-readable units of meaning. A Pinecone article echoes this: if a chunk "makes sense without the surrounding context to a human, it will make sense to the language model as well". So avoid arbitrary cuts in the middle of ideas – use natural boundaries (sentences, paragraphs, or sections).

**Effects on LLM Context Window**: The chunk size also determines how much text you'll feed into the LLM. Language models have limited context windows (e.g. 4k, 16k tokens), and we want to avoid hitting those limits or wasting tokens on irrelevant material. A key point is that including too much irrelevant text can confuse the model or degrade the quality of its output. Modern LLMs will try to use all provided context; if 30% of the context is off-topic, the model might generate a diluted or tangential response, or even incorporate some of that noise into the answer. One study put it plainly: "Giving the LLM too much irrelevant text can confuse it or cause it to include unrelated info." Therefore, smaller, focused chunks can improve the effective use of the context window – you pack more relevant info into the prompt per token consumed.

However, if chunks are too small, you might need to retrieve many of them to cover a broad query, which in turn could fill up the context window with disjoint pieces. Extremely fragmented context might increase the cognitive load on the model to synthesize information from many pieces, and potentially on the user if the assistant is showing many snippets. It can also lead to the model missing the bigger picture if it sees facts in isolation. There's also the "lost in the middle" phenomenon for long contexts: LLMs sometimes struggle to retain or give weight to information buried in the middle of a very long prompt. If you retrieve a few very large chunks, you risk important details being lost in a sea of text in the prompt. So, even though new LLMs have huge contexts (hundreds of thousands of tokens in latest models), feeding entire documents isn't necessarily optimal. It can be better to retrieve the key passages (plus a little surrounding context) so that the relevant information stands out.

**Overlap and Redundancy**: Overlapping chunks (where contiguous chunks share some sentences/tokens) are commonly used to ensure recall – important details at the boundary of one chunk likely appear in the next. Overlap improves the chance that a relevant snippet isn't missed due to unlucky cut-off. But overlap means redundant tokens in the index and in retrieval results, which can hurt token-level precision and IoU. If your evaluation shows that overlap is retrieving the same relevant sentence twice, you're wasting part of the LLM's context on duplication. One metric from Chroma's report, Precision_Ω, assumes "if all chunks containing any part of the relevant excerpt are retrieved" – essentially an upper-bound precision if recall were perfect. The gap between this and actual precision tells how much redundancy you added to capture all info. In practice, a small overlap (15–30% of chunk length) is often a reasonable trade-off, but tune it as needed. If your content has lots of small paragraphs, maybe no overlap is needed (each paragraph self-contained). If it's more continuous text, overlap can help.

**Chunking Strategy Innovations**: Beyond fixed-size splitting, recent approaches attempt smarter chunking:

• **Semantic Chunking**: E.g. Chroma's ClusterSemanticChunker groups sentences that are semantically related until a token limit is reached. This can yield variable-length chunks that hopefully each stay on one subtopic. In their evaluation, this method achieved the highest precision and IoU (i.e. very efficient, little junk per relevant token) but slightly lower recall – likely because making very tight semantic clusters can leave some relevant info out if it didn't cluster well. This suggests semantic chunking can reduce noise significantly (each chunk is topically cohesive), but one must be careful that the clustering doesn't over-segment the text and miss connections.

• **LLM-based Chunking**: Using the LLM itself to decide chunk boundaries (e.g. prompt "split this document into self-contained sections"). Chroma's LLMChunker tried this and found it competitive with other methods. An LLM might understand the structure (like an intro, body, conclusion) better than a simple rule-based splitter.

• **Dynamic Passage Retrieval (Sentence-Windowing)**: A clever two-stage idea is to use very small chunks for initial retrieval to get precise hits, then expand those hits in context before generation. Harshad (2024) calls this "Context Enrichment with Sentence Window Retrieval." In practice, you first index at the sentence level (so you retrieve, say, a single pertinent sentence). Once you have a relevant hit, you pull in its neighbors (the surrounding few sentences from the original document) to give the LLM more context to work with. This way, you get the precision of tiny chunks but also provide the model with the necessary broader context for understanding. The retrieved result presented to the LLM is a "window" around the key sentence, rather than the key sentence alone. This method can reduce the chance of the model misunderstanding a snippet out of context, and can include important qualifying information that a one-liner might miss. It's like saying: find the needle, then also bring a bit of the hay around it so the model knows where it came from. Initial experiments showed that this can balance precision and context, often yielding better answers than either strategy alone.

• **Rerankers and Filters**: Another tactic is accepting that you might retrieve somewhat larger or more numerous chunks, but then filter or rerank them before passing to the LLM. For example, you could retrieve 10 chunks with high recall, then use a cross-encoder reranker (a smaller transformer or even GPT-4) to pick the best 3 to actually feed the LLM. Superlinked found that using a lightweight re-ranker (TinyBERT cross-encoder) significantly improved final retrieval quality without huge compute cost. A reranker essentially re-checks relevance with full context, and can discard irrelevant or redundant chunks. This means you can err on the side of recalling too much with bigger chunks or lower embedding thresholds, and trust the reranker to tighten precision later. For instance, Masood (2023) suggests if latency allows, it's sometimes better to retrieve a few extra "noisy" chunks and let the LLM figure it out, rather than possibly missing something. But he also cautions that too much noise will confuse the model, so a reranker is helpful to "only send the good stuff" to the final prompt. Rerankers effectively increase precision at the last step, which can compensate if your chunking was a bit broad. They also allow larger chunk sizes to be used safely – e.g. you might retrieve whole paragraphs for recall, but the reranker will make sure only truly relevant ones go into the answer context. The trade-off is added complexity and computation, but many open-source options (like BM25+cross-encoder or ColBERT) are available to integrate.

**Practical Tuning Tips**: Finding the optimal chunk size/strategy is usually an empirical process:

1. **Experiment with Multiple Sizes**: Don't rely on a default. Try splitting your content into, say, 100-token, 200-token, 500-token chunks (with a fixed overlap like 50 tokens), and measure retrieval metrics on a validation set. You might find, for example, that recall@5 jumps from 70% to 90% when going from 100 to 200 tokens, but going to 500 only adds 2% more recall while precision drops a lot. That indicates diminishing returns on chunk size.

2. **Consider Content Structure**: If your cached pages are well-structured (headings, sections), leverage that. Perhaps chunk by top-level headings or paragraphs. If the pages are text-dense or semi-structured (like forums or transcripts), a uniform token length chunking might be simpler. As Pinecone's guide notes, the type of data matters – e.g. smaller docs may not need chunking at all, while long ones might have logical sub-sections you should preserve. Always ensure not to cut off tables, code blocks, or list items in awkward ways if those appear in your data.

3. **Use Overlap Judiciously**: A little overlap can drastically improve recall, especially if you opt for smaller chunks. But track how much your effective token usage goes up. If you overlap by 50%, you're indexing almost twice as many tokens (with duplicates) which affects vector DB size and possibly retrieval speed. Monitor the token-level precision/IoU – if it plummets with heavy overlap, maybe your chunks are too small or your overlap too high. Overlap is most useful when information spans across boundaries; if your content is such that each segment is relatively self-contained, you can reduce overlap.

4. **Advanced Methods on a Case-by-Case Basis**: Semantic or LLM-driven chunking can be beneficial, but also carry overhead (needing an extra step or more complex logic). They may shine on very unstructured text. For example, if your web pages contain sections that are topically all over the place (like a single page with unrelated Q&A sections), a semantic chunker could separate those by topic better than a blind token splitter. But if your pages are a coherent article, a simpler strategy might suffice. The Superlinked finding suggests starting simple (sentence or paragraph based) and only moving to complex strategies if needed.

5. **Continuous Evaluation and Iteration**: As Harshad concluded, there is no one-size-fits-all chunk size. Your ideal chunking may evolve as your knowledge base grows or your query patterns change. Regularly evaluate your retrieval effectiveness (both via automated metrics and user feedback) after any significant content update. If you notice the LLM is often getting confused or not finding answers, revisit the chunking. In a PKM, monitor how often users have to refine queries or scroll through retrievals – signs your chunks might be too large (if they contain a lot of irrelevant text each) or too small (if users get only bits and have to open multiple snippets). Be ready to re-tune overlap or size as needed.

6. **User Experience Matters**: Chunk size isn't just a back-end concern – it affects the UI. If your system shows the user the retrieved passages, consider what chunk size is reader-friendly. Large chunks mean the user must wade through a long excerpt (cognitive load), whereas very small ones might lack context and force the user to piece together fragments. Some teams address this by highlighting query-relevant text within chunks or by collapsing/expanding content. As a rule, try to present chunks in a way that the user can quickly grasp why it was retrieved. This often aligns with the chunk making sense on its own, as discussed. Transparent highlighting of the answer-bearing sentence within a larger chunk can also combine the benefits of context + precision.

By following these practices – measuring retrieval and generation outcomes and iteratively tuning chunking – you can significantly boost a RAG system's real-world performance. For example, you might find that switching from a 1000-token splitter to a smarter sentence-window approach yields a 20% gain in precision without sacrificing recall, and users report the suggestions feel "much more on point". In one case, simply changing the chunking strategy improved recall by up to 9% and reduced irrelevant tokens, directly translating to less distraction for the LLM. The investment in evaluation pays off: you end up with a PKM assistant that retrieves the right knowledge and delivers it in a way that the user (and the LLM) can easily digest, increasing both the utility and trust in the system.

## Conclusion

Building an effective RAG-powered PKM system means balancing a three-way relationship between the user, the retriever, and the generator. Robust evaluation is your compass: use IR metrics (augmented for chunk-level behavior) to ensure the retriever finds what's needed, and use generation metrics (automatic and human) to ensure the LLM's output is accurate and helpful. The chunk size and strategy form the backbone of this pipeline – they determine what the retriever can find and what context the LLM sees. Optimising chunking is a process of tuning and trade-offs: find a size that captures complete thoughts (to aid retrieval and comprehension) without straying too far afield. Leverage overlap, semantic splitting, or hybrid strategies when appropriate, and always test changes against your evaluation criteria. Don't forget the human element: measure how real users fare with the system. Are they more productive? Do they trust the suggested content? These qualitative aspects often validate the quantitative metrics.

In a dynamic environment of cached web pages, where content varies from well-structured articles to noisy forum posts, a one-off solution won't suffice. But by applying the latest best practices – from token-level recall measurements to innovative chunking techniques – and by continuously refining based on feedback, you can develop a RAG system that truly augments the user's knowledge management process. It will fetch information effectively, present it in digestible chunks, and allow the LLM to generate responses that are reliable and context-rich. The result is a PKM assistant that feels less like a gimmick and more like a trustworthy research companion: always ready with the right snippet at the right time, and smart enough to stay on track.

## References

- Evaluating Chunking Strategies for Retrieval | Chroma Research  
  https://research.trychroma.com/evaluating-chunking

- Mastering RAG: How To Evaluate LLMs For RAG  
  https://galileo.ai/blog/how-to-evaluate-llms-for-rag

- 5 key features and benefits of retrieval augmented generation (RAG) | The Microsoft Cloud Blog  
  https://www.microsoft.com/en-us/microsoft-cloud/blog/2025/02/13/5-key-features-and-benefits-of-retrieval-augmented-generation-rag/

- Evaluating the Optimal Document Chunk Size for a RAG Application | by Harshad | Medium  
  https://harshadsuryawanshi.medium.com/evaluating-the-optimal-document-chunk-size-for-a-rag-application-9cb482365bbf

- An evaluation of RAG Retrieval Chunking Methods | VectorHub by Superlinked  
  https://superlinked.com/vectorhub/articles/evaluation-rag-retrieval-chunking-methods

- Chunking Strategies for LLM Applications | Pinecone  
  https://www.pinecone.io/learn/chunking-strategies/

- Optimizing Chunking, Embedding, and Vectorization for Retrieval-Augmented Generation | by Adnan Masood, PhD. | Medium  
  https://medium.com/@adnanmasood/optimizing-chunking-embedding-and-vectorization-for-retrieval-augmented-generation-ea3b083b68f7
